{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpful Articles:\n",
    "\n",
    "#https://towardsdatascience.com/beyond-the-lyrics-the-intersection-of-music-and-data-visualization-4a71039f447c\n",
    "#https://towardsdatascience.com/how-we-used-nltk-and-nlp-to-predict-a-songs-genre-from-its-lyrics-54e338ded537\n",
    "\n",
    "#http://cs229.stanford.edu/proj2017/final-reports/5241796.pdf\n",
    "\n",
    "#https://algorithmia.com/blog/using-machine-learning-for-sentiment-analysis-a-deep-dive\n",
    "#https://towardsdatascience.com/naive-bayes-document-classification-in-python-e33ff50f937e\n",
    "#https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34\n",
    "#https://colab.research.google.com/drive/1ixOZTKLz4aAa-MtC6dy_sAvc9HujQmHN#scrollTo=ytg-0FTHG7ik\n",
    "#https://perun.pmf.uns.ac.rs/radovanovic/dmsem/cd/install/Weka/doc/pubs/2004/KibriyaAI04-MultinomialNBRevisited.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Song Genre from Song Lyrics\n",
    "\n",
    "The data is an accumulation of 380,000+ lyrics from the website MetroLyrics.com. We have multiple artists in 10 varying genres ranging from Pop & Rock to Indie & Folk. The songs range from years 1960 to 2016. \n",
    "\n",
    "The goal of the project was, as said in title, to predict genre based on the lyrics. In order to tackle this problem, this involved doing natural language processing in order to effectively analyze the lyrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Manda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Manda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Manda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Manda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95761 rows dropped (no lyrics)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('lyrics.csv', delimiter=',')\n",
    "df1.dataframeName = 'lyrics.csv'\n",
    "emptyLyrics = len(df1)\n",
    "df1 = df1[df1['lyrics']!='instrumental'].dropna()\n",
    "emptyLyrics -= len(df1)\n",
    "print(str(emptyLyrics) + \" rows dropped (no lyrics)\")\n",
    "\n",
    "#drop 'Not Available' genre\n",
    "df1 = df1[df1['genre']!='Not Available'].dropna()\n",
    "df1['lyrics'].dropna(inplace=True)\n",
    "df1['lyrics'].dropna(inplace=True)\n",
    "\n",
    "#Change all the text to lower case. \n",
    "df1['lyrics'] = [entry.lower() for entry in df1['lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = df1.sample(frac = 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Tokenization\n",
    "\n",
    "Basically, what word tokenization does is splits the text into individual words. For instance, for the lyrics \"oh baby, how you doing?\" into a list with the tokens ['oh','baby', \",\",'how','you','doing','?']. As seen here, it splits punctuations into their own tokens which will be removed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['lyrics']= [word_tokenize(entry) for entry in df1['lyrics']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing the Lyrics and Removing Stop Words from Lyrics\n",
    "\n",
    "##### Stop Words:\n",
    "As most of us are aware, there are words in songs that have no significance to the listener for understanding the song. Some examples include \"la la\", \"oh\", \"ooo\", \"na\", etc. It is important that we remove these words along with other words that the NLTK package deems as stopwords such as \"the\", \"an\", \"a\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the index (because we took random sample of data) for next function\n",
    "df1 = df1.drop(columns = ['index'])\n",
    "df1 = df1.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "customStopWords = [\"'s\", \"n't\", \"'m\", \"'re\", \"'ll\",\"'ve\",\"...\", \"ä±\", \"''\", '``',\\\n",
    "                  '--', \"'d\", 'el', 'la', 'chorus', 'verse', 'oh', 'la', 'ya', 'na', 'wo', 'wan', 'Chorus', 'Verse',\n",
    "                  'ca', 'cuz', '[Verse 1:]', '[Intro:]', '[Chorus]', '\\n', 's', 't', 'n', 'don',\n",
    "                  'ya','aah','ye','hey','ba','da','buh','duh','doo','oh','ooh','woo','uh','hoo','ah','yeah',\n",
    "                   'oo','la','chorus','beep','ha']\n",
    "\n",
    "stopWords = stopwords.words('english') + customStopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Lemmatization:\n",
    "WordNetLemmatizer() enables the process of converting a word to its base form. This involves grouping together different inflected forms of a word, while still keeping the context of the word. Words with multiple variations, but with similar meanings, can be analysed as a single item. An example of this is \"better\" to \"good\", \"saying\" to \"say\", and \"heard\" to \"hear\".\n",
    "\n",
    "WordNetLemmatizer() takes a part of speech parameter: here it is \"pos_tag\". This is important for lemmatization so the computer can recognize the context of the word to lemmatize it properly. \n",
    "\n",
    "If a speech parameter is not supplied, the default is \"noun.\" This means that an attempt will be made to find the closest noun, which can create trouble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#labeling words as their respective parts of speech\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "#segmenting by parts of speech\n",
    "for index,entry in enumerate(df1['lyrics']):\n",
    "    Final_words = []\n",
    "    verb_words = []\n",
    "    adv_words = []\n",
    "    noun_words = []\n",
    "    adj_words = []\n",
    "    #WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        if word not in stopwords.words('english') and word not in stopWords and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "        df1.loc[index,'lyrics_final'] = str(Final_words)\n",
    "        \n",
    "        if word not in stopwords.words('english') and word not in stopWords and word.isalpha() and 'NN' in tag:\n",
    "            noun_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            noun_words.append(noun_Final)\n",
    "        df1.loc[index,'lyrics_noun'] = str(noun_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data for Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(df1['lyrics_final'],df1['genre'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is TFIDF Vectorizer - Measuring Originality\n",
    "\n",
    "### TFIDF - term-frequency x inverse document-frequency\n",
    "\n",
    "TFIDF takes an occurence of a token (i.e. a lemmatized word) in the data and scales down the impact of these tokens that occur very frequently in our data. In our example, this means a lemmatized word would be \"less informative\" if it occured more frequently across all genres. A key example of this would be the word \"love\".\n",
    "\n",
    "The tf-idf is a statistic that increases with the number of times a word appears in the document (lyrics of a song), penalized by the number of genres in the data that contain that word.\n",
    "\n",
    "However, if a lemmatized word appeared more frequently in a particular genre it would be more valuable/original, and we would  capture that originality which will help us predict the genre of that that group of words. \n",
    "\n",
    "\n",
    "------------------\n",
    "\n",
    "max_df - ignoring terms that have a document frequency higher than 0.5   \n",
    ".fit() - fits the vocabulary and term frequencies of word-vector   \n",
    ".transform() - using \"fit()\" calculation parameters, apply the transformation to a dataset.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(sublinear_tf=True, max_features=500, max_df=0.5) #igoring\n",
    "Tfidf_vect.fit(df1['lyrics_final'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a TFIDF Vector?\n",
    "\n",
    "First Column: Document index (song lyrics) \n",
    "\n",
    "Second Column: Specific word-vector index (lemmatized word in lyrics)  \n",
    "\n",
    "Third Column: TFIDF score (\"originality score\") for word in lyrics\n",
    "\n",
    "----------------\n",
    "\n",
    "Here is an example of what a tfidf sparse matrix looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (63, 0)\t0.29619966120734137\n",
      "  (88, 0)\t0.2329065782522921\n",
      "  (142, 0)\t0.1118067687214563\n",
      "  (193, 0)\t0.24542051581260635\n",
      "  (227, 0)\t0.15556056436107607\n",
      "  (270, 0)\t0.260991868315882\n",
      "  (364, 0)\t0.36506060953644315\n",
      "  (374, 0)\t0.3806046422433339\n",
      "  (420, 0)\t0.08462105943768704\n",
      "  (443, 0)\t0.08600433945050576\n",
      "  (473, 0)\t0.09718290825405236\n",
      "  (493, 0)\t0.09454518568139618\n",
      "  (562, 0)\t0.11171370566583995\n",
      "  (609, 0)\t0.14494775945472493\n",
      "  (619, 0)\t0.17205466904809663\n",
      "  (631, 0)\t0.08207772112174122\n",
      "  (674, 0)\t0.1726111973479978\n",
      "  (723, 0)\t0.14199423484291174\n",
      "  (850, 0)\t0.41936202081384105\n",
      "  (855, 0)\t0.17222275569383147\n",
      "  (922, 0)\t0.11386297191683073\n",
      "  (939, 0)\t0.09092201587821115\n",
      "  (961, 0)\t0.20846012864704794\n",
      "  (1011, 0)\t0.11263148123692943\n",
      "  (1029, 0)\t0.09753028472012273\n",
      "  (1101, 0)\t0.2031039363502327\n",
      "  (1121, 0)\t0.15286874128829503\n",
      "  (1375, 0)\t0.138341401157753\n"
     ]
    }
   ],
   "source": [
    "print(Test_X_Tfidf[:,199:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here a more tangible look at the tfidf sparse matrix using .get_feature_names(). get_features_names() shows, in alphabetical order, the words of the word-vectors that would be in column 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acerca',\n",
       " 'activo',\n",
       " 'aleja',\n",
       " 'alto',\n",
       " 'amarte',\n",
       " 'amigo',\n",
       " 'arena',\n",
       " 'arrepentidos',\n",
       " 'arte',\n",
       " 'atenciones',\n",
       " 'atento',\n",
       " 'ayudar',\n",
       " 'aãºn',\n",
       " 'besa',\n",
       " 'bien',\n",
       " 'botella',\n",
       " 'buena',\n",
       " 'buenos',\n",
       " 'caballos',\n",
       " 'camino',\n",
       " 'carros',\n",
       " 'ciegas',\n",
       " 'conmigo',\n",
       " 'conocer',\n",
       " 'correr',\n",
       " 'corridos',\n",
       " 'cry',\n",
       " 'cualquier',\n",
       " 'cuidado',\n",
       " 'dedos',\n",
       " 'deja',\n",
       " 'deportivos',\n",
       " 'elegante',\n",
       " 'ella',\n",
       " 'empinarle',\n",
       " 'encuentro',\n",
       " 'entre',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'escuchar',\n",
       " 'esfuerzo',\n",
       " 'esta',\n",
       " 'estatura',\n",
       " 'existe',\n",
       " 'familia',\n",
       " 'fue',\n",
       " 'gano',\n",
       " 'gente',\n",
       " 'girl',\n",
       " 'go',\n",
       " 'guerrero',\n",
       " 'gusta',\n",
       " 'gustan',\n",
       " 'gusto',\n",
       " 'hacerla',\n",
       " 'hay',\n",
       " 'hombre',\n",
       " 'importante',\n",
       " 'las',\n",
       " 'le',\n",
       " 'lo',\n",
       " 'logrado',\n",
       " 'los',\n",
       " 'love',\n",
       " 'luis',\n",
       " 'luna',\n",
       " 'luz',\n",
       " 'madre',\n",
       " 'manos',\n",
       " 'marea',\n",
       " 'mirada',\n",
       " 'moldearla',\n",
       " 'moriria',\n",
       " 'mucha',\n",
       " 'muchas',\n",
       " 'mujeres',\n",
       " 'muy',\n",
       " 'mãºsica',\n",
       " 'nadie',\n",
       " 'never',\n",
       " 'nivel',\n",
       " 'obra',\n",
       " 'olvidare',\n",
       " 'olvido',\n",
       " 'otra',\n",
       " 'padre',\n",
       " 'para',\n",
       " 'parte',\n",
       " 'pasar',\n",
       " 'pedro',\n",
       " 'pendiente',\n",
       " 'pero',\n",
       " 'piel',\n",
       " 'present',\n",
       " 'problemas',\n",
       " 'querida',\n",
       " 'quiero',\n",
       " 'rato',\n",
       " 'real',\n",
       " 'realidad',\n",
       " 'respetado',\n",
       " 'run',\n",
       " 'san',\n",
       " 'sangre',\n",
       " 'secuestro',\n",
       " 'show',\n",
       " 'si',\n",
       " 'siempre',\n",
       " 'siento',\n",
       " 'su',\n",
       " 'sus',\n",
       " 'tan',\n",
       " 'tenerla',\n",
       " 'tengo',\n",
       " 'tiene',\n",
       " 'tomarla',\n",
       " 'trabajo',\n",
       " 'tuvo',\n",
       " 'tãº',\n",
       " 'un',\n",
       " 'uuhh',\n",
       " 'va',\n",
       " 'veces',\n",
       " 'ver',\n",
       " 'vez',\n",
       " 'viejo']"
      ]
     },
     "execution_count": 938,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#these are examples of what the second column represents\n",
    "Tfidf_vect.fit(df1['lyrics_final'].head(2)).get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes - Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score TFIDF: 50.824175824175825\n"
     ]
    }
   ],
   "source": [
    "#TFIDF\n",
    "Naive = naive_bayes.MultinomialNB(alpha = .01)\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "print(\"Naive Bayes Accuracy Score TFIDF:\",accuracy_score(predictions_NB, Test_Y)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score:  53.57142857142857\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "print(\"SVM Accuracy Score: \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
